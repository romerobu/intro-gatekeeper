= Gatekeeper
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:icons: font
endif::[]
// End: Enable admonition icons


// Create the Contents here
toc::[]

== Introduction to OPA

== What is Open Policy Agent?

Open Policy Agent is an open source Cloud Native Foundation project which aims to implement policies
enforcement for creating, updating and deleting Kubernetes objects operations.
Open Policy Agent is based on Admission Controller which intercepts Kubernetes API calls to verify the
requested objects against the created policies.
OPA utilizes a declarative Rego language specifically designed for policies implementation that allows to
iterate through and transform structured documents. It’s based on Datalog, a query language but Rego
extends it.

== What is Gatekeeper?

Gatekeeper is a project that provides integration between OPA and Kubernetes, adding some additional
features.
It is a project that bases the integration on the use of Validation Webhooks that apply OPA policies
through Gatekeeper’s own Custom Resource Definitions. And these webhooks fire every time a
Kubernetes object is updated, created or deleted.
On the other hand, Gatekeeper integrates an audit feature that registers all the events related to the
status of a constraint by enabling periodic evaluation of resources against the policies. Audit configuration
is defined when creating the Gatekeeper resource and according to this configuration will be based on
auditing from replicated data in caché or via Kubernetes API.

== Install Gatekeeper operator

Gatekeeper operator can be installed via Red Hat Marketplace or by creating a Subscription resource.

 - Console:

   * Navigate to Operators marketplace
   * Install "Gatekeeper Operator"
   * Create Gatekeeper resource to configure the audit and admission webhook features. This object is used by
the operator for deploying the respective components with the specific configuration.

 - Subscription:
   
   * Create Subscription resource as in config/install-operator.yaml
   * Create Gatekeeper resource as in config/create-gatekeeper.yaml

Additionally in this repo you can find a script called "setup.sh" which creates all the namespaces requires for this demo lab plus the operator resources for installation and constraints.

[source, bash]
----
./setup.sh
----

Apart from Openshift Gatekeeper operator, you can install this feature using helm charts available https://open-policy-agent.github.io/gatekeeper/website/docs/install[here].

== Gatekeeper operator features

=== Constraint Templates

A ConstraintTemplate is a Custom Resource Definition that describes the Rego
rule to implement a constraint and the Kind of constraint to be instanced. This Rego rule defines the
logic policy to be enforced by iterating through the json object requested while the instance of the
constraint will define input parameters and namespaces to apply this constraint.

Here you can find information about the https://www.openpolicyagent.org/docs/latest/policy-reference/[policy reference].

=== Constraints

Constraints are instances of the template scheme definition. After creating a
ConstraintTemplate, the Kind specified on the scheme can be instanced. When creating a constraint,
on spec section are defined the enforcement mode, match namespaces and kinds and input
parameters to evaluate the implemented policy.

Constraints define:
 
 - Parameters: threshold values.
 - Kinds: list of object to which the constraint will apply.
 - Scope: cluster-scoped or namespace-scope resources affected by the constraint. This works together with namespaces excluded by config file.
 - Namespace: apply the constraint to an specific namespace.
 - Excluded namespace: apply the constraint to a non listed namespace.
 - Label selector: apply constraint to these labeled resources.
 - Namespace selector: apply constraint to specific synced namespaces.

=== Admission webhook

Gatekeeper is implemented as a set pg  Kubernetes admission webhooks, one for checking a request against the installed constraints and another one for checking labels on namespace requests to bypass certain constraints. 

The ValidationWebhookConfiguration object allows to modify admission webhooks on the fly, so after
creating the Gatekeeper object a new ValidationWebhookConfiguration fully editable will be created. By
default, this object forces to audit all objects created in Openshift.

It is possible to modify this object to limit the number of objects and namespaces audited in order to
improve the Gatekeeper service performance.

Overriding this configuration is not covered on this repo but you can find the information https://open-policy-agent.github.io/gatekeeper/website/docs/customize-admission[here].

You can check current admission hook configuration with this command:


[source, bash]
----
oc get ValidatingWebhookConfiguration gatekeeper-validating-webhook-configuration -o yaml
----

=== Config

Config Gatekeeper object is an object to apply the general configuration to control specific cache sync,
match and validation settings.
ConfigSpec section at Gatekeeper object yaml defines the desired state of Config and its features and is
not defined by default.

One of the main purposes of this resource is the possibility of excluding certaing namespaces for policy implementing. You can take a look to the documentation here https://open-policy-agent.github.io/gatekeeper/website/docs/exempt-namespaces[here].

=== Audit, Syncing and Debugging

==== Audit

Open Policy Agent runs on the cluster as an instance while Audit feature is implemented as an Audit
Controller which periodically queries the OPA audit endpoint, and evaluates watched object against rego
policies, thus writing results to constraint resources. The Source of truth for objects to be audited can be
via discovery client or from caché.
Discovery client list all objects matching all kinds and once all are listed, exclude those whose namespaces
are excluded at config level resource. Then every object is reviewed against the audit endpoint of OPA
instance and response return values will be populated to constraint status.
If the audit process is performed

Audit configuration values like memory consumption, scope or limits can be
overrided to improve performance. Those are defined when creating the Gatekeeper resource. Audit
feature must be properly configured to get a good performance and ensure availability of the service.

Some of these values are:

- Constraint violations limit: default to 20.
- Audit chunk size: default to infinite. To limit memory consumption of the auditing Pod.
- Audit interval: default to 60 seconds. 
- Audit from cache: default to false. 

==== Debugging

Constraints must specify an enforcement action which is deny by default. Other option is dryrun mode which allows to test constraint without making actual changes while are registered as violations in the audit status section.
Logs details are configured when creating the Gatekeeper resource. Log levels ranges between DEBUG, INFO, WARNING and ERROR.

Additionally in Config resource you can enable traces for some resources and a specific user. These traces will be logged to the stdout of the Gatekeeper controller.

==== Syncing

Config resource defines a list of object to be synced by defining group, version and kind. Once this list of objects is synced, they can be accesed via data inventory document following this structure:

 -  data.inventory.cluster-group-kind-name
 -  data.inventory.namespace-group-kind-name

This feature is interesting not only for its potential to improve performance but it allows to implement rules which require access to other resources than the one observed directly by the rule.

== Use cases

Here you can find some basic examples about how to implement restrictions and how they work.
If you run the ./setup.sh script you will deploy a list of resources that will be tested by creating good and bad resources to test positive and negative violation cases.

Here you can check webhook and audit configuration values as well as validation.

[source, bash]
----
oc get Subscription gatekeeper-operator-product -n openshift-operators -o yaml
----

[source, bash]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gatekeeper-operator-product
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: gatekeeper-operator-product
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: gatekeeper-operator-product.v0.1.2
----

[source, bash]
----
oc get gatekeeper gatekeeper -o yaml
----


[source, bash]
----
apiVersion: operator.gatekeeper.sh/v1alpha1
kind: Gatekeeper
metadata:
  name: gatekeeper
spec:
  validatingWebhook: Enabled
  webhook:
    logLevel: DEBUG
    replicas: 2
  image:
    image: >-
      registry.redhat.io/rhacm2/gatekeeper-rhel8@sha256:5e66cd510a80ef5753c66c6b50137de0093fe75c0606f5f8ce4afce7d7bca050
  audit:
    logLevel: DEBUG
    replicas: 1
----

[source, bash]
----
oc get config.config.gatekeeper.sh/config -o yaml -n openshift-gatekeeper-system
----

[source, bash]
----
apiVersion: config.gatekeeper.sh/v1alpha1
kind: Config
metadata:
  name: config
  namespace: "openshift-gatekeeper-system"
spec:
  sync:
    syncOnly:
      - group: ""
        version: "v1"
        kind: "Pod"            
      - group: "*"
        version: "v1"
        kind: "Deployment"
      - group: ""
        version: "*"
        kind: "Namespace"
      - group: ""
        version: "v1"
        kind: "ResourceQuota"        
  match:
    - excludedNamespaces: ["gatekeeper-project-excluded"]
      processes: ["*"]
----

Later on you will deploy a series of constraints and templates tested in the next steps.

=== Max pod replicas

With this rule you are limiting the amount of replicas for a deployment. This constraint is limited to namespace "gatekeeper-project" and resource "Deployment". Enforcement action is "Deny" and max replicas allowed is 3.

This means you won't be able to create a deployment with more replicas than allowed and you will be prompted with error message "Deployment %v pods is higher than the maximum allowed of 3".

If you try to create a deployment in a different namespace (not excluded by Config) this constraint won't apply.

==== Create a valid deployment.

[source, bash]
----
oc apply -f examples/deployment-yes.yaml
----

Expected result: Ok.

==== Create a non-valid deployment within "gatekeeper-project" namespace.

[source, bash]
----
oc apply -f examples/deployment-no-project.yaml
----

Expected result: Fail.

==== Create a non-valid deployment in a non-excluded namespace "gatekeeper-system".

[source, bash]
----
oc apply -f examples/deployment-no.yaml -n gatekeeper-system
----

Expected result: Ok.

==== Create a non-valid deployment in an excluded namespace.
* Depende de si este namespace está excluido en config

[source, bash]
----
oc apply -f examples/deployment-no.yaml -n gatekeeper-project-excluded
----

Expected result: Ok.

=== Max containers resources

In this case, constraint is limitating the resources a Pod can request (memory and cpu) within the whole cluster less excluded namespace "gatekeeper-project-excluded" namespace. As memory and cpu resources request can be measured in different units it would be useful to estandarize this calculation to be able to convert constraint limit unit to a different one.

==== Create valid Pod.

[source, bash]
----
oc apply -f examples/pod-yes.yaml
----

Expected result: Ok.

==== Create non-valid Pod.

[source, bash]
----
oc apply -f examples/pod-no.yaml -n gatekeeper-project
----

Expected result: Fail.

==== Create non-valid Pod in "gatekeeper-project-excluded" namespace.

As this namespace is excluded for this constraint, you should be able to create pod which exceed request parameters.

[source, bash]
----
oc apply -f examples/pod-no.yaml -n gatekeeper-project-excluded
----

Expected result: Ok.

==== Create a non-valid Pod in a different non-excluded namespace.

[source, bash]
----
oc apply -f examples/pod-no.yaml -n gatekeeper-system
----

Expected result: Fail.

==== Create a Deployment with request values higher than specified by Constraint. 

This deployment will create a ReplicaSet resource which won't be able to scale as Pod doesn't fulfill requirements.
If you go to ReplicaSet events, you should be prompted with an error message as your deployment is trying to create Pods which request higher values than allowed.

[source, bash]
----
oc apply -f examples/deployment-pod-no.yaml -n gatekeeper-project
----

Expected result: Fail.

Para solucionar problema de las unidades: 

oc delete k8smaxrequests.constraints.gatekeeper.sh/pod-max-requests +
oc delete constraintTemplate k8smaxrequests +


=== Missing labels

For this use case you will test a Constraint which limits the use of labels. This constraints forces you to create deployments including a required label called "gatekeeper" otherwise you won't be able to create any deployment.

Furthermore we are going to test how reactive gatekeeper can become. This means if webhook is able to detect existing violations for a recently created constraint.

==== Create a non-valid Deployment within "gatekeeper-system" namespace.

[source, bash]
----
oc apply -f examples/deployment-label-no.yaml
----

Expected result: Ok.

==== Deploy missing labels constraint.

[source, bash]
----
oc apply -f constraintTemplate/K8sRequiredLabels.yaml
oc apply -f constraints/K8sRequiredLabels.yaml
----

We need to wait audit interval time and check constraint status.

[source, bash]
----
oc get k8srequiredlabels.constraints.gatekeeper.sh/required-label-deployment -o yaml

...
  totalViolations: 1
  violations:
  - enforcementAction: deny
    kind: Deployment
    message: 'you must provide labels: {"gatekeeper"}'
    name: example-no
    namespace: gatekeeper-system
...
----

Then try to create a couple of valid a non valid deployments to test working constraints.
You won't be able to create a new deployment without "gatekeeper" label as before.

Finally if you delete the existing deployment which violates missing labels constraint, total violations will be down to O.

[source, bash]
----
oc delete deployment example-no  -n gatekeeper-system
oc get k8srequiredlabels.constraints.gatekeeper.sh/required-label-deployment -o yaml

...
  totalViolations: 0
...
----

==== Create a valid Deployment within "gatekeeper-system" namespace.

[source, bash]
----
oc apply -f examples/deployment-label-yes.yaml
oc apply -f examples/deployment-label-no-2.yaml
----

Expected result: Ok.
Expected result: Fail.

==== Create a Pod missing the required label. As constraint is auditing only Deployment resources, you should be able to create it.

[source, bash]
----
oc apply -f examples/pod-label-yes.yaml
----

Expected result: Ok.

=== Deploy constraint using synced values

In this example we are going to use Audit feature to access more resources synced in cache apart from the resource under test. This means that all the resources specified at Config (syncOnly) can be accessed via data.properties.

This constraint within "gatekeeper-resourcequota" namespace won't allow to deploy a pod into a namespace without an existing resource quota. Constraint matches new pods, and the template defines the Rego rule to check existing resource quotas via data.properties.

==== Create a non valid pod

[source, bash]
----
oc apply -f examples/pod-rq-no.yaml
----

Expected result: Fail.

==== Create a valid pod

[source, bash]
----
oc apply -f examples/pod-rq-yes.yaml
----

Expected result: Ok.
