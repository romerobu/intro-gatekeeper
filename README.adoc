= Gatekeeper
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnums: 
:source-highlighter: pygments
:imagesdir: images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:icons: font
endif::[]
// End: Enable admonition icons


// Create the Contents here
toc::[]

== Introduction to OPA

== What is Gatekeeper?

Gatekeeper is a "validating webhook that enforces CRD-based policies executed by Open Policy Agent". Gatekeeper allows to define admission and constraints scenarios via templates and to see what resources are violated.

Gatekeeper is offered as an Operator on Red Hat Openshift marketplace and includes a parameterized policy library, constraints and constraint templates resources and audit feature.

Even though Gatekeeper works according to default values, these can be overrided. By default if webhook is not working as expected, constraints will not be enforced.

== Install Gatekeeper operator

Gatekeeper operator can be installed via Red Hat Marketplace or by creating a Subscription resource.

 - Console:

   * Navigate to Operators marketplace
   * Install "Gatekeeper Operator"
   * Create Gatekeeper resource

 - Subscription:
   
   * Create Subscription resource as in config/install-operator.yaml
   * Create Gatekeeper resource as in config/create-gatekeeper.yaml

Additionally in this repo you can find a script called "setup.sh" which creates all the namespaces requires for this exercise plus the operator resources for installation and constraints.

[source, bash]
----
./setup.sh
----

Apart from Openshift Gatekeeper operator, you can install this feature using helm charts available https://open-policy-agent.github.io/gatekeeper/website/docs/install[here].

== Gatekeeper operator features

=== Constraint Templates

Constraint templates define the pattern of the constraint and the Rego rule. Most of these patterns are defined by iterating through the resource the constraint is auditing and comparing this value with the threshold specified by the constraint. You can use logical operator, type transformation and looping as part of it.

Here you can find information about the https://www.openpolicyagent.org/docs/latest/policy-reference/[policy reference].

=== Constraints

Constraint resources defined how the template must be enforced as it specifies the value to be hooked and how. Constraints must be created after the template is implemented. 
Constraints define:
 
 - Parameters: threshold values.
 - Kinds: list of object to which the constraint will apply.
 - Scope: cluster-scoped or namespace-scope resources affected by the constraint. This works together with namespaces excluded by config file.
 - Namespace: apply the constraint to an specific namespace.
 - Excluded namespace: apply the constraint to a non listed namespace.
 - Label selector: apply constraint to these labeled resources.
 - Namespace selector: apply constraint to specific synced namespaces.

=== Admission webhook

Gatekeeper is a Kubernetes admission webhook resource which defines two different admission webhooks, one for checking a request against the installed constraints and another one for checking labels on namespace requests to bypass certain constraints.

Webhooks values like timeouts and failure policy can be configured to ignore certain type of errors, allow request in specific conditions, tune performance or customize availability. Changing these configuration is not covered on this repo but you can find the information https://open-policy-agent.github.io/gatekeeper/website/docs/customize-admission[here].

You can check current admission hook configuration with this command:


[source, bash]
----
oc get ValidatingWebhookConfiguration gatekeeper-validating-webhook-configuration -o yaml
----

=== Config

Completar todas las configs que se pueden poner

https://open-policy-agent.github.io/gatekeeper/website/docs/exempt-namespaces

=== Audit, Syncing and Debugging

==== Audit

Audit feature register all the events related to the status of a constraint by enabling periodic evaluation of resources against the policies.
Audit configuration values like memory consumption, scope or limits can be overrided to improve performance. Those are defined as part of the Config resource previously mentioned.
Some of these values are:

- Constraint violations limit: default to 20.
- Audit chunk size: default to infinite. To limit memory consumption of the auditing Pod.
- Audit interval: default to 60 seconds. 
- Audit from cache: default to false. Audit will request each resource from the Kubernetes API during each cycle of the audit unless you specify this flad and define a match kind resource, in this case it will be audited from cache. Auditing from cache saves time as it doesn't have to audit all resources in the cluster. Not defining match kind resources is equal to set this flag to false.

==== Debugging

Constraints must specify an enforcement action which is deny by default. Other option is dryrun mode which allows to test constraint without making actual changes while are registered as violations in the audit status section.
Logs details are configured when creating the Gatekeeper resource. Log levels ranges between DEBUG, INFO, WARNING and ERROR.

Additionally in Config resource you can enable traces for some resources and a specific user. These traces will be logged to the stdout of the Gatekeeper controller.

==== Syncing

Config resource defines a list of object to be synced by defining group, version and kind. Once this list of objects is synced, they can be accesed via data inventory document following this structure:

 -  data.inventory.cluster-group-kind-name
 -  data.inventory.namespace-group-kind-name

This feature is interesting not only for its potential to improve performance but it allows to implement rules which require access to other resources than the one observed directly by the rule.

== Use cases

Here you can find some basic examples about how to implement restrictions and how they work.
If you run the ./setup.sh script you will deploy a list of resources that will be tested by creating good and bad resources to test positive and negative violation cases.

Here you can check webhook and audit configuration values as well as validation.

[source, bash]
----
oc get Subscription gatekeeper-operator-product -n openshift-operators -o yaml
----

[source, bash]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gatekeeper-operator-product
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: gatekeeper-operator-product
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: gatekeeper-operator-product.v0.1.2
----

[source, bash]
----
oc get gatekeeper gatekeeper -o yaml
----


[source, bash]
----
apiVersion: operator.gatekeeper.sh/v1alpha1
kind: Gatekeeper
metadata:
  name: gatekeeper
spec:
  audit:
    replicas: 1
    logLevel: INFO
    auditInterval: "30"
    auditChunkSize: 500
    constraintViolationsLimit: 5
    auditFromCache: Enabled
    # auditMatchKindOnly: true
  validatingWebhook: Enabled
  webhook:
    logLevel: INFO
    replicas: 2
  image:
    image: >-
      registry.redhat.io/rhacm2/gatekeeper-rhel8@sha256:5e66cd510a80ef5753c66c6b50137de0093fe75c0606f5f8ce4afce7d7bca050
----

[source, bash]
----
oc get config.config.gatekeeper.sh/config -o yaml -n openshift-gatekeeper-system
----

[source, bash]
----
apiVersion: config.gatekeeper.sh/v1alpha1
kind: Config
metadata:
  name: config
  namespace: "gatekeeper-system"
spec:
  sync:
    syncOnly:
      - group: ""
        version: "v1"
        kind: "Namespace"
      - group: ""
        version: "v1"
        kind: "Pod"
      - group: "*"
        version: "v1"
        kind: "Deployment"
  match:
    - excludedNamespaces: ["gatekeeper-project-excluded"]
      processes: ["webhook"]     
  validation:  
    traces:
      - user: cromerob
        kind:
          group: ""
          version: "v1"
          kind: "Namespace"
          dump: "All" 
----

Later on you will deploy a series of constraints and templates tested in the next steps.

=== Max pod replicas

With this rule you are limiting the amount of replicas for a deployment. This constraint is limited to namespace "gatekeeper-project" and resource "Deployment". Enforcement action is "Deny" and max replicas allowed is 3.

This means you won't be able to create a deployment with more replicas than allowed and you will be prompted with error message "Deployment %v pods is higher than the maximum allowed of 3".

If you try to create a deployment in a different namespace (not excluded by Config) this constraint won't apply.

==== Create a valid deployment.

[source, bash]
----
oc apply -f examples/deployment-yes.yaml
----

Expected result: Ok.

==== Create a non-valid deployment within "gatekeeper-project" namespace.

[source, bash]
----
oc apply -f examples/deployment-no.yaml -n gatekeeper-project
----

Expected result: Fail.

==== Create a non-valid deployment in a non-excluded namespace "gatekeeper-system".

[source, bash]
----
oc apply -f examples/deployment-no.yaml -n gatekeeper-system
----

Expected result: Ok.

==== Create a non-valid deployment in an excluded namespace.


[source, bash]
----
oc apply -f examples/deployment-no.yaml -n gatekeeper-project-excluded
----

Expected result: Ok.

=== Max containers resources

In this case, constraint is limitating the resources a Pod can request (memory and cpu) within the whole cluster less excluded namespace "gatekeeper-project-excluded" namespace. As memory and cpu resources request can be measured in different units it would be useful to estandarize this calculation to be able to convert constraint limit unit to a different one.

==== Create valid Pod.

[source, bash]
----
oc apply -f examples/pod-yes.yaml
----

Expected result: Ok.

==== Create non-valid Pod.

[source, bash]
----
oc apply -f examples/pod-no.yaml -n gatekeeper-project
----

Expected result: Fail.

==== Create non-valid Pod in "gatekeeper-project-excluded" namespace.

As this namespace is excluded for this constraint, you should be able to create pod which exceed request parameters.

[source, bash]
----
oc apply -f examples/pod-no.yaml -n gatekeeper-project-excluded
----

Expected result: Ok.

==== Create a non-valid Pod in a different non-excluded namespace.

[source, bash]
----
oc apply -f examples/pod-no.yaml -n gatekeeper-system
----

Expected result: Fail.

==== Create a Deployment with request values higher than specified by Constraint. 

This deployment will create a ReplicaSet resource which won't be able to scale as Pod doesn't fulfill requirements.
If you go to ReplicaSet events, you should be prompted with an error message as your deployment is trying to create Pods which request higher values than allowed.

[source, bash]
----
oc apply -f examples/deployment-pod-no.yaml -n gatekeeper-project
----

Expected result: Fail.

=== Missing labels

For this use case you will test a Constraint which limits the use of labels. This constraints forces you to create deployments including a required label called "gatekeeper" otherwise you won't be able to create any deployment.

Furthermore we are going to test how reactive gatekeeper can become. This means if webhook is able to detect existing violations for a recently created constraint.

==== Create a non-valid Deployment within "gatekeeper-system" namespace.

[source, bash]
----
oc apply -f examples/deployment-label-no.yaml
----

Expected result: Ok.

==== Deploy missing labels constraint.

[source, bash]
----
oc apply -f constraintTemplate/K8sRequiredLabels.yaml
oc apply -f constraints/K8sRequiredLabels.yaml
----

We need to wait audit interval time and check constraint status.

[source, bash]
----
oc get k8srequiredlabels.constraints.gatekeeper.sh/required-label-deployment -o yaml

...
  totalViolations: 1
  violations:
  - enforcementAction: deny
    kind: Deployment
    message: 'you must provide labels: {"gatekeeper"}'
    name: example-no
    namespace: gatekeeper-system
...
----

Then try to create a couple of valid a non valid deployments to test working constraints.
You won't be able to create a new deployment without "gatekeeper" label as before.

Finally if you delete the existing deployment which violates missing labels constraint, total violations will be down to O.

[source, bash]
----
oc delete deployment example-no
oc get k8srequiredlabels.constraints.gatekeeper.sh/required-label-deployment -o yaml

...
  totalViolations: 0
...
----

==== Create a valid Deployment within "gatekeeper-system" namespace.

[source, bash]
----
oc apply -f examples/deployment-label-yes.yaml
oc apply -f examples/deployment-label-no-2.yaml
----

Expected result: Ok.
Expected result: Fail.

==== Create a Pod missing the required label. As constraint is auditing only Deployment resources, you should be able to create it.

[source, bash]
----
oc apply -f examples/pod-label-yes.yaml
----

Expected result: Ok.

=== Deploy constraint using synced values

In this example we are going to use Audit feature to access more resources synced in caché apart from the resource under test. This means that all the resources specified at Config (syncOnly) can be accessed via data.properties.

This constraint within "gatekeeper-system" namespace won't allow to deploy a sole replica set without a deployment. Constraint matches new replica sets, and the template defines the Rego rule to compare deployment names to replica set names via data.properties.

==== Create a valid replica set with a deployment

[source, bash]
----
oc apply -f examples/pod-label-yes.yaml
----

Expected result: Ok.

==== Create a non valid replica set without a deployment

[source, bash]
----
oc apply -f examples/pod-label-no.yaml
----

Expected result: Fail.
